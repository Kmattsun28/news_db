import feedparser
import os
import requests
import pdfplumber
from pptx import Presentation
from datetime import datetime, timedelta
from app.script.db import SessionLocal
from app.script.models import NewsArticle
from app.script.debug import debug_printer as d
from app.script.utils_scraper import extract_article_text, detect_currency_tags
from app.script.summarizer import summarize_text
# Finnhub APIã®è¿½åŠ 
from app.script.finnhub_news import fetch_finnhub_forex_news

# RSS_FEEDSã®å®šç¾©ã¯ get_optimized_rss_feeds() é–¢æ•°å†…ã«ç§»å‹•ã—ã¾ã—ãŸ



def is_recent_article(published_parsed, hours_back=24):
    """
    è¨˜äº‹ãŒæŒ‡å®šã—ãŸæ™‚é–“ä»¥å†…ã«å…¬é–‹ã•ã‚ŒãŸã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
    
    Args:
        published_parsed: feedparserã®published_parsedãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        hours_back: ä½•æ™‚é–“å‰ã¾ã§ã®è¨˜äº‹ã‚’æœ‰åŠ¹ã¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 24æ™‚é–“ï¼‰
    
    Returns:
        bool: æŒ‡å®šæ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®å ´åˆTrueã€ãã‚Œä»¥å¤–ã¯False
    """
    if not published_parsed:
        return False
    
    try:
        article_time = datetime(*published_parsed[:6])
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        is_recent = article_time >= cutoff_time
        
        # if not is_recent:
        #     d.print(f"â° Article too old: {article_time.strftime('%Y-%m-%d %H:%M:%S')} (cutoff: {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')})", level="debug")
        
        return is_recent
    except Exception as e:
        d.print(f"Error checking article date: {e}", level="warning")
        return False


def extract_pdf_text(url: str) -> str:
    try:
        response = requests.get(url)
        with open("/tmp/tmp.pdf", "wb") as f:
            f.write(response.content)
        text = ""
        with pdfplumber.open("/tmp/tmp.pdf") as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text.strip()
    except Exception as e:
        d.print(f"PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def extract_pptx_text(url: str) -> str:
    try:
        response = requests.get(url)
        with open("/tmp/tmp.pptx", "wb") as f:
            f.write(response.content)
        prs = Presentation("/tmp/tmp.pptx")
        text = ""
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + "\n"
        return text.strip()
    except Exception as e:
        d.print(f"PPTXæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return ""
    
def extract_xlsx_text(url: str) -> str:
    import subprocess
    tmp_xlsx = "/tmp/tmp.xlsx"
    tmp_pdf = "/tmp/tmp_xlsx.pdf"
    try:
        response = requests.get(url)
        with open(tmp_xlsx, "wb") as f:
            f.write(response.content)
        # LibreOfficeã§xlsxâ†’pdfå¤‰æ›
        subprocess.run([
            "soffice", "--headless", "--convert-to", "pdf", "--outdir", "/tmp", tmp_xlsx
        ], check=True)
        # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        text = ""
        if os.path.exists(tmp_pdf):
            with pdfplumber.open(tmp_pdf) as pdf:
                for page in pdf.pages:
                    text += page.extract_text() or ""
        return text.strip()
    except Exception as e:
        d.print(f"XLSXæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return ""
    finally:
        for path in [tmp_xlsx, tmp_pdf]:
            if os.path.exists(path):
                os.remove(path)

def fetch_and_store_rss():
    session = SessionLocal()
    d.print_ts(f"<<< scheduled task: fetch_and_store_rss >>>", level='debug')
    
    total_processed = 0
    total_added = 0
    batch_size = 10  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨­å®š
    current_batch = 0

    try:
        # 1. æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å…ˆã«å‡¦ç†ï¼ˆåŠ¹ç‡çš„ï¼‰
        time_filtered_feeds, standard_feeds = get_optimized_rss_feeds()
        
        # æ™‚é–“æŒ‡å®šå¯èƒ½ãªãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å‡¦ç†ï¼ˆGoogle Newsãªã©ï¼‰
        d.print("Processing time-filtered RSS feeds...", level="info")
        for category, urls in time_filtered_feeds.items():
            d.print(f"Fetching time-filtered RSS feeds for category: {category} contains {len(urls)}", level="debug", output_path="./data/fetch_and_store_rss.log")
            for url in urls:
                try:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’å–å¾—
                    cache_info = get_feed_cache_info(url)
                    
                    # åŠ¹ç‡çš„RSSå–å¾—ï¼ˆetag/modifiedä½¿ç”¨ï¼‰
                    feed, new_etag, new_modified = fetch_rss_with_caching(
                        url, 
                        cache_info['etag'], 
                        cache_info['modified']
                    )
                    
                    if feed is None:  # 304 Not Modified or error
                        continue
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’æ›´æ–°
                    save_feed_cache_info(url, new_etag, new_modified)
                    
                    d.print(f"Processing time-filtered feed: {url}, entries: {len(feed.entries)}", level="debug")
                    
                    # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãŸã‚ã€å…¨è¨˜äº‹ã‚’å‡¦ç†
                    for entry in feed.entries:
                        try:
                            total_processed += 1
                            
                            published = datetime(*entry.published_parsed[:6]) if entry.get("published_parsed") else datetime.now()
                            exists = session.query(NewsArticle).filter_by(title=entry.title, published=published).first()
                            if exists:
                                d.print(f"â© skip article: {entry.title[:50]}... (already exists)", output_path="./data/fetch_and_store_rss.log")
                                continue

                            # è¨˜äº‹å‡¦ç†ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                            ext = os.path.splitext(entry.link)[1].lower()
                            full_text = ""
                            
                            try:
                                if ext == ".pdf":
                                    d.print(f"Processing PDF: {entry.link}", level="warning")
                                    full_text = extract_pdf_text(entry.link)
                                elif ext in [".ppt", ".pptx"]:
                                    d.print(f"Processing PPTX: {entry.link}", level="warning")
                                    full_text = extract_pptx_text(entry.link)
                                elif ext in [".xls", ".xlsx"]:
                                    d.print(f"Processing XLSX: {entry.link}", level="warning")
                                    full_text = extract_xlsx_text(entry.link)
                                else:
                                    full_text = extract_article_text(entry.link)
                            except Exception as scrape_error:
                                d.print(f"Scraping failed for {entry.link}: {scrape_error}", level="warning")
                                full_text = ""

                            if not full_text:
                                full_text = entry.get("summary", entry.get("title", ""))

                            try:
                                summary = summarize_text(full_text)
                                currency_tags = detect_currency_tags(full_text)
                            except Exception as ai_error:
                                d.print(f"AI processing failed for {entry.title[:50]}...: {ai_error}", level="warning")
                                summary = entry.get("summary", entry.get("title", ""))
                                currency_tags = []

                            article = NewsArticle(
                                category=category,
                                title=entry.title,
                                summary=summary,
                                url=entry.link,
                                published=published,
                                currency_tags=currency_tags
                            )
                            session.add(article)
                            total_added += 1
                            current_batch += 1
                            d.print(f"âœ… Added time-filtered RSS article: {entry.title[:50]}...", level="debug")
                            
                            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ä¸­é–“ã‚³ãƒŸãƒƒãƒˆ
                            if current_batch >= batch_size:
                                try:
                                    session.commit()
                                    d.print(f"ğŸ“¦ Batch commit: {current_batch} articles saved", level="info")
                                    current_batch = 0
                                except Exception as commit_error:
                                    d.print(f"âŒ Batch commit failed: {commit_error}", level="error")
                                    session.rollback()
                                    current_batch = 0
                            
                        except Exception as entry_error:
                            d.print(f"Error processing entry {entry.get('title', 'Unknown')[:50]}...: {entry_error}", level="error")
                            continue
                            
                except Exception as feed_error:
                    d.print(f"Error processing time-filtered feed {url}: {feed_error}", level="error")
                    continue
        
        # 2. æ¨™æº–ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆæ™‚é–“æŒ‡å®šä¸å¯ï¼‰ã‚’å‡¦ç†
        d.print("Processing standard RSS feeds with post-filtering...", level="info")
        for category, urls in standard_feeds.items():
            d.print(f"Fetching standard RSS feeds for category: {category} contains {len(urls)}", level="debug", output_path="./data/fetch_and_store_rss.log")
            for url in urls:
                try:
                    # æ¨™æº–çš„ãªRSSå–å¾—
                    feed = feedparser.parse(url)
                    d.print(f"Processing standard feed: {url}, entries: {len(feed.entries)}", level="debug")
                    
                    for entry in feed.entries:
                        try:
                            total_processed += 1
                            
                            # è¨˜äº‹ã®å…¬é–‹æ—¥ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆéå»24æ™‚é–“ä»¥å†…ã®ã‚‚ã®ã®ã¿å‡¦ç†ï¼‰
                            if not is_recent_article(entry.get("published_parsed"), hours_back=24):
                                # d.print(f"â° Skipping old article: {entry.title[:50]}... (published too long ago)", level="debug", output_path="./data/fetch_and_store_rss.log")
                                continue
                            
                            published = datetime(*entry.published_parsed[:6]) if entry.get("published_parsed") else datetime.now()
                            exists = session.query(NewsArticle).filter_by(title=entry.title, published=published).first()
                            if exists:
                                d.print(f"â© skip article: {entry.title[:50]}... (already exists)", output_path="./data/fetch_and_store_rss.log")
                                continue

                            # RSSã‚¨ãƒ³ãƒˆãƒªã®ãƒªãƒ³ã‚¯ã®ç¨®é¡ã§å‡¦ç†ã‚’åˆ†å² http, pdf, pptx, xlsx
                            ext = os.path.splitext(entry.link)[1].lower()
                            full_text = ""
                            
                            try:
                                if ext == ".pdf":
                                    d.print(f"Processing PDF: {entry.link}", level="warning")
                                    full_text = extract_pdf_text(entry.link)
                                elif ext in [".ppt", ".pptx"]:
                                    d.print(f"Processing PPTX: {entry.link}", level="warning")
                                    full_text = extract_pptx_text(entry.link)
                                elif ext in [".xls", ".xlsx"]:
                                    d.print(f"Processing XLSX: {entry.link}", level="warning")
                                    full_text = extract_xlsx_text(entry.link)
                                else:
                                    full_text = extract_article_text(entry.link)
                            except Exception as scrape_error:
                                d.print(f"Scraping failed for {entry.link}: {scrape_error}", level="warning")
                                full_text = ""

                            if not full_text:
                                full_text = entry.get("summary", entry.get("title", ""))

                            try:
                                summary = summarize_text(full_text)
                                currency_tags = detect_currency_tags(full_text)
                            except Exception as ai_error:
                                d.print(f"AI processing failed for {entry.title[:50]}...: {ai_error}", level="warning")
                                summary = entry.get("summary", entry.get("title", ""))
                                currency_tags = []

                            article = NewsArticle(
                                category=category,
                                title=entry.title,
                                summary=summary,
                                url=entry.link,
                                published=published,
                                currency_tags=currency_tags
                            )
                            session.add(article)
                            total_added += 1
                            current_batch += 1
                            d.print(f"âœ… Added standard RSS article: {entry.title[:50]}...", level="debug")
                            
                            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ä¸­é–“ã‚³ãƒŸãƒƒãƒˆ
                            if current_batch >= batch_size:
                                try:
                                    session.commit()
                                    d.print(f"ğŸ“¦ Batch commit: {current_batch} articles saved", level="info")
                                    current_batch = 0
                                except Exception as commit_error:
                                    d.print(f"âŒ Batch commit failed: {commit_error}", level="error")
                                    session.rollback()
                                    current_batch = 0
                            
                        except Exception as entry_error:
                            d.print(f"Error processing standard entry {entry.get('title', 'Unknown')[:50]}...: {entry_error}", level="error")
                            continue
                            
                except Exception as feed_error:
                    d.print(f"Error processing standard feed {url}: {feed_error}", level="error")
                    continue

        # æ®‹ã‚Šã®è¨˜äº‹ã‚’ã‚³ãƒŸãƒƒãƒˆ
        if current_batch > 0:
            try:
                session.commit()
                d.print(f"ğŸ“¦ Final batch commit: {current_batch} articles saved", level="info")
            except Exception as commit_error:
                d.print(f"âŒ Final batch commit failed: {commit_error}", level="error")
                session.rollback()

        d.print(f"ğŸ†— All RSS feeds processed. Total processed: {total_processed}, Added: {total_added}", level="info")
        
    except Exception as e:
        session.rollback()
        d.print(f"Error in fetch_and_store_rss: {e}", level="error")
    finally:
        session.close()


def fetch_and_store_all_news():
    """
    ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ï¼ˆRSS + Finnhub APIï¼‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦DBã«ä¿å­˜
    
    RSSåé›†æœŸé–“: éå»24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿
    Finnhubåé›†æœŸé–“: éå»30åˆ†é–“
    """
    d.print_ts("<<< scheduled task: fetch_and_store_all_news >>>", level='info')
    
    total_new_articles = 0
    
    # 1. æ—¢å­˜ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆéå»24æ™‚é–“ä»¥å†…ï¼‰
    d.print("=== RSS News Collection (Last 24 hours) ===", level="info")
    try:
        fetch_and_store_rss()
        d.print("âœ… RSS news collection completed", level="info")
    except Exception as e:
        d.print(f"âŒ RSS news collection failed: {e}", level="error")
    
    # 2. Finnhub APIã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆéå»30åˆ†é–“ï¼‰
    d.print("=== Finnhub News Collection (Last 30 minutes) ===", level="info")
    try:
        finnhub_count = fetch_finnhub_forex_news(limit=50, minutes_back=30)
        total_new_articles += finnhub_count
        d.print(f"âœ… Finnhub news collection completed. New articles: {finnhub_count}", level="info")
    except Exception as e:
        d.print(f"âŒ Finnhub news collection failed: {e}", level="error")
    
    d.print(f"ğŸ All news collection completed. Total new articles from Finnhub: {total_new_articles}", level="info")
    return total_new_articles


def generate_time_filtered_rss_urls():
    """
    æ™‚é–“ç¯„å›²ã‚’æŒ‡å®šã—ãŸRSS URLã‚’å‹•çš„ã«ç”Ÿæˆ
    Google Newsãªã©ä¸€éƒ¨ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã¯æ™‚é–“ç¯„å›²ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾å¿œ
    """
    from datetime import datetime, timedelta
    
    # 24æ™‚é–“å‰ã®æ—¥æ™‚ã‚’å–å¾—ï¼ˆé–‹å§‹æ—¥ã®å‰æ—¥ï¼‰
    start_time = datetime.now() - timedelta(hours=24)
    after_param = (start_time - timedelta(days=1)).strftime("%Y-%m-%d")  # é–‹å§‹æ—¥ã®å‰æ—¥
    before_param = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")  # çµ‚äº†æ—¥ã®ç¿Œæ—¥
    
    # æ™‚é–“æŒ‡å®šå¯èƒ½ãªRSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆGoogle Newsï¼‰
    time_filtered_feeds = {
        "google_news": [
            f"https://news.google.com/rss/search?q=USD+JPY+exchange+rate+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=EUR+USD+exchange+rate+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=Federal+Reserve+rate+decision+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=US+inflation+CPI+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=nonfarm+payrolls+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=ECB+Lagarde+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=EUR+inflation+after:{after_param}+before:{before_param}&hl=en-US&gl=US&ceid=US:en",
            f"https://news.google.com/rss/search?q=æ—¥éŠ€+ç‚ºæ›¿ä»‹å…¥+after:{after_param}+before:{before_param}&hl=ja&gl=JP&ceid=JP:ja",
            f"https://news.google.com/rss/search?q=å††å®‰+after:{after_param}+before:{before_param}&hl=ja&gl=JP&ceid=JP:ja",
            f"https://news.google.com/rss/search?q=æ—¥ç±³é‡‘åˆ©å·®+after:{after_param}+before:{before_param}&hl=ja&gl=JP&ceid=JP:ja",
        ]
    }
    
    return time_filtered_feeds


def get_optimized_rss_feeds():
    """
    æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œãƒ»éå¯¾å¿œã‚’åˆ†ã‘ã¦RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
    """
    # æ™‚é–“æŒ‡å®šå¯èƒ½ãªãƒ•ã‚£ãƒ¼ãƒ‰
    time_filtered = generate_time_filtered_rss_urls()
    
    # æ™‚é–“æŒ‡å®šä¸å¯ã®ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå¾“æ¥é€šã‚Šå¾Œã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰
    standard_feeds = {
        # -------------------------------------------------------------------
        # ä¸­å¤®éŠ€è¡Œãƒ»æ”¿åºœç³»æ©Ÿé–¢ï¼ˆç‚ºæ›¿ãƒ»æ”¿ç­–ç™ºè¡¨ï¼‰
        # -------------------------------------------------------------------
        "official_sources": [
            "https://www.boj.or.jp/rss/whatsnew.rdf",                          # æ—¥æœ¬éŠ€è¡Œ
            "https://www.mof.go.jp/public_relations/rss.xml",                 # è²¡å‹™çœï¼ˆç‚ºæ›¿ä»‹å…¥å«ã‚€ï¼‰
            "https://www.federalreserve.gov/feeds/pressreleases.xml",         # FRB
            "https://home.treasury.gov/news/press-releases/feed",             # ç±³è²¡å‹™çœ
            "https://www.ecb.europa.eu/press/rss/pr.xml",                     # æ¬§å·ä¸­å¤®éŠ€è¡Œ
            # "https://www.ecb.europa.eu/stats/eurofxref/eurofxref-usd.rss",    # EUR/USDç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼ˆECBå…¬å¼ï¼‰
            # "https://www.bundesbank.de/rss/bbk_en_news.xml",                  # ãƒ‰ã‚¤ãƒ„é€£é‚¦éŠ€è¡Œï¼ˆè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
        ],
        
        # -------------------------------------------------------------------
        # å°‚é–€FXãƒ‹ãƒ¥ãƒ¼ã‚¹/åˆ†æï¼ˆUSD, JPY, EURä¸­å¿ƒï¼‰
        # -------------------------------------------------------------------
        "forex_feeds": [
            # "https://www.forexlive.com/feed/",                                 # ForexLiveï¼ˆå³æ™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
            # "https://www.fxstreet.com/rss/news",                               # FXStreet - ãƒ‹ãƒ¥ãƒ¼ã‚¹
            # "https://www.fxstreet.com/rss/analysis",                           # FXStreet - åˆ†æ
            # "https://www.fxstreet.com/rss/forex-news",                         # FXStreet - ç‚ºæ›¿é™å®š
            # "https://www.fxstreet.com/rss/forex-technical-analysis",           # FXStreet - ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æ
            # "https://www.investing.com/rss/news_forex.xml",                   # Investing.com - FXãƒ‹ãƒ¥ãƒ¼ã‚¹
            # "https://www.investing.com/rss/technical.xml",                    # Investing.com - ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«
            # "https://www.actionforex.com/feed",                                # ActionForex
        ],
        
        # -------------------------------------------------------------------
        # å›½éš›çµŒæ¸ˆãƒ»é‡‘èå ±é“ï¼ˆUSDãƒ»EURæ”¿ç­–ã«å½±éŸ¿ï¼‰
        # -------------------------------------------------------------------
        "international_media_en": [
            # "https://feeds.bloomberg.com/markets/news.rss",                   # Bloomberg - Markets
            # "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",                 # Wall Street Journal - Markets
            # "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml",     # NYTimes - Business
            "https://www.economist.com/finance-and-economics/rss.xml",       # The Economist - Finance
        ],
        
        # -------------------------------------------------------------------
        # å›½å†…ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆç‚ºæ›¿/é‡‘è/çµŒæ¸ˆï¼‰
        # -------------------------------------------------------------------
        "domestic_media_jp": [
            "https://www.nikkei.com/rss/newselement/Nni01.xml",              # æ—¥çµŒ - ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹
            # "https://www.nikkei.com/rss/market/market.xml",                  # æ—¥çµŒ - ãƒãƒ¼ã‚±ãƒƒãƒˆ
            # "https://toyokeizai.net/list/feed/rss",                          # æ±æ´‹çµŒæ¸ˆ - ç·åˆRSS
            # "https://www3.nhk.or.jp/rss/news/cat0.xml",                      # NHK - ç·åˆçµŒæ¸ˆ
        ]
    }
    
    return time_filtered, standard_feeds


def fetch_rss_with_caching(url, etag=None, modified=None):
    """
    RSSå–å¾—æ™‚ã«etagã¨modifiedãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡çš„ã«å–å¾—
    å‰å›å–å¾—æ™‚ã‹ã‚‰å¤‰æ›´ãŒãªã„å ´åˆã¯304 Not ModifiedãŒè¿”ã•ã‚Œã‚‹
    
    Args:
        url: RSS URL
        etag: å‰å›å–å¾—æ™‚ã®etag
        modified: å‰å›å–å¾—æ™‚ã®modifiedæ™‚åˆ»
    
    Returns:
        tuple: (feed, new_etag, new_modified)
    """
    try:
        # etagã¨modifiedãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚‰ã‚’æŒ‡å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        feed = feedparser.parse(url, etag=etag, modified=modified)
        
        # 304 Not Modified ã®å ´åˆã€æ–°ã—ã„è¨˜äº‹ãªã—
        if feed.status == 304:
            d.print(f"ğŸ“„ No new articles in feed: {url}", level="debug")
            return None, etag, modified
        
        # æ–°ã—ã„etagã¨modifiedã‚’ä¿å­˜
        new_etag = feed.get('etag')
        new_modified = feed.get('modified_parsed')
        
        return feed, new_etag, new_modified
        
    except Exception as e:
        d.print(f"Error fetching RSS with caching {url}: {e}", level="error")
        return None, None, None


def get_feed_cache_info(url):
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’å–å¾—ï¼ˆå®Ÿè£…ä¾‹ï¼‰
    å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    # ç°¡æ˜“å®Ÿè£…ï¼šãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ï¼ˆå®Ÿç”¨ã§ã¯æ°¸ç¶šåŒ–ãŒå¿…è¦ï¼‰
    cache_store = getattr(get_feed_cache_info, 'cache', {})
    return cache_store.get(url, {'etag': None, 'modified': None})


def save_feed_cache_info(url, etag, modified):
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’ä¿å­˜
    """
    if not hasattr(save_feed_cache_info, 'cache'):
        save_feed_cache_info.cache = {}
    save_feed_cache_info.cache[url] = {'etag': etag, 'modified': modified}